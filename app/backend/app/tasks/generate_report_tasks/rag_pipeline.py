"""
RAG pipeline for generating report sections with hybrid approach support
"""
from typing import List, Dict, Any
from app.core.config import settings
from app.db.database_service import get_database_service
import logging
import time
import asyncio
from app.utils.llm_provider import (
    create_llm_instance,
    generate_embedding,
    generate_query_embedding
)
from app.utils.hybrid_search import hybrid_search_documents, generate_context_from_results
from app.utils.quality_controller import AutomaticQualityController
from app.utils.rag_cache import cached

# Configure the logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize database service
db_service = get_database_service()

def get_adaptive_threshold(section_id: str, query_count: int) -> float:
    """
    Return adaptive similarity threshold based on section and query complexity.
    
    Args:
        section_id: The ID of the section being generated
        query_count: Number of queries being used for this section
        
    Returns:
        Adaptive similarity threshold between 0.3 and 0.7
    """
    # Base thresholds for different section types
    base_thresholds = {
        "belastbaarheid": 0.6,  # Higher threshold for medical content
        "visie_ad": 0.55,       # Medium for analysis
        "matching": 0.5,        # Lower for job matching
        "samenvatting": 0.45,   # Lowest for general summaries
        "conclusie": 0.55,      # Medium for conclusions
        "vraagstelling": 0.5,   # Medium for question formulation
        "gesprek": 0.45,        # Lower for conversation reports
        "loonwaarde": 0.5,      # Medium for wage value
        "prognose": 0.5         # Medium for prognosis
    }
    
    # Get base threshold or use default
    base = base_thresholds.get(section_id, 0.5)
    
    # Adjust based on query count - more queries = lower threshold
    # This allows more results when using multiple diverse queries
    query_adjustment = min(0.1, 0.02 * (query_count - 1))
    
    # Calculate final threshold with bounds checking
    final_threshold = max(0.3, min(0.7, base - query_adjustment))
    
    logger.info(f"Adaptive threshold for {section_id}: {final_threshold:.2f} (base: {base:.2f}, queries: {query_count})")
    
    return final_threshold

@cached("hsearch", ttl=21600)  # 6 hours - cache hybrid search results
async def get_relevant_chunks(section_id: str, document_ids: List[str], case_id: str, limit: int = 15):
    """
    Retrieve relevant document chunks for a specific report section using hybrid search
    with advanced query formulation for Dutch labor expert reports.
    Uses a multi-query strategy with domain-specific queries for better recall.

    Cached for 6 hours to balance freshness and performance.
    Cache key includes: section_id, document_ids, case_id, limit
    """
    try:
        # Advanced query formulation for Dutch labor expert contexts
        # Formulated based on domain knowledge and real-world work reintegration documents
        query_mapping = {
            "samenvatting": [
                # Personal and demographic information
                "persoonsgegevens leeftijd geslacht opleiding werkervaring cliÃ«nt",
                # Background and case history
                "voorgeschiedenis arbeidssituatie ziektegeschiedenis werknemer",
                # Reason for assessment
                "aanleiding arbeidsdeskundig onderzoek arbeidsmogelijkheden re-integratie",
                # Current status
                "huidige situatie functioneren dagbesteding activiteiten belastbaarheid"
            ],
            "belastbaarheid": [
                # Physical capacity with specific details
                "fysieke belastbaarheid tillen dragen duwen trekken bukken zitten staan lopen",
                # Cognitive and mental capacity
                "mentale belastbaarheid concentratie aandacht geheugen informatieverwerking stress",
                # Social functioning
                "sociale belastbaarheid communicatie samenwerking instructies feedback",
                # Functional capacity assessment
                "functionele mogelijkheden inzetbaarheid belastingpunten FML Functionele Mogelijkhedenlijst"
            ],
            "visie_ad": [
                # Professional assessment
                "visie arbeidsdeskundige professionele beoordeling analyse",
                # Conclusions and advice
                "conclusies adviezen aanbevelingen arbeidsdeskundig rapport",
                # Work capacity and reintegration prospects
                "arbeidsmogelijkheden werkhervatting re-integratie toekomstperspectief participatie",
                # Limitations and impact assessment
                "beperkingen mogelijkheden impact arbeidsvermogen functioneren"
            ],
            "matching": [
                # Suitable jobs and functions
                "passend werk geschikte functies beroepen arbeidsmarkt",
                # Workplace adjustments and tools
                "werkaanpassingen hulpmiddelen ondersteuning ergonomische maatregelen",
                # Requirements and conditions
                "randvoorwaarden werkhervatting arbeidsparticipatie re-integratie",
                # Job criteria and requirements
                "functie-eisen arbeidsmogelijkheden belasting geschikt werk"
            ]
        }

        # Special section handling for custom sections
        additional_sections = {
            "loonwaarde": [
                "loonwaarde productiviteit arbeidsprestatie rendement",
                "benutbare mogelijkheden tempo kwaliteit inzetbaarheid",
                "arbeidsvermogen prestatie-indicatoren productienorm"
            ],
            "prognose": [
                "prognose herstel verbetering verwachting toekomst",
                "behandeltraject interventies herstelkansen",
                "re-integratievooruitzichten werkhervatting perspectief"
            ]
        }

        # Merge additional sections into the main mapping
        query_mapping.update(additional_sections)

        # Determine which queries to use
        if section_id in query_mapping:
            queries = query_mapping[section_id]
        else:
            # Generic query with basic search terms for unknown sections
            queries = [
                f"informatie over {section_id} arbeidsdeskundig onderzoek",
                f"{section_id} rapport arbeidsdeskundige expertise",
                f"{section_id} professionele beoordeling werkhervatting"
            ]

        # Add universal search terms for small document sets
        if len(document_ids) <= 2:
            queries.append("belangrijke informatie arbeidsdeskundig rapport")

        # Calculate adaptive similarity threshold based on section and query count
        adaptive_threshold = get_adaptive_threshold(section_id, len(queries))
        
        # Set a larger limit for hybrid search to get a good pool of results
        # We'll filter down to the requested limit after
        hybrid_limit = min(30, limit * 3)

        # Run searches for all queries and collect results
        all_results = []

        for query in queries:
            logger.info(f"Running hybrid search for query: {query[:50]}...")
            # TODO: Cache search results for case_id + section_id + query combination
            search_results = await hybrid_search_documents(
                query=query,
                case_id=case_id,
                document_ids=document_ids,
                limit=hybrid_limit // len(queries),
                similarity_threshold=adaptive_threshold,  # Use adaptive threshold
                # Distribute results across strategies
                strategy_distribution={
                    "direct_llm": 0.4,   # Prioritize direct results
                    "hybrid": 0.4,       # Equal priority for hybrid
                    "full_rag": 0.2      # Less priority for full RAG
                }
            )

            # If search was successful, add results to our collection
            if search_results.get("success", False):
                # Transform search results to match the expected format
                for result in search_results.get("results", []):
                    result_dict = {
                        "id": result.get("chunk_id", ""),
                        "document_id": result.get("document_id", ""),
                        "content": result.get("content", ""),
                        "metadata": result.get("metadata", {}),
                        "similarity": result.get("similarity", 0.0),
                        "strategy": result.get("strategy", "unknown"),
                        "query": query  # Store the query that produced this result
                    }
                    all_results.append(result_dict)

        # Remove duplicates based on chunk ID
        seen_ids = set()
        unique_results = []

        for result in all_results:
            result_id = result.get("id", "")
            if result_id and result_id not in seen_ids:
                seen_ids.add(result_id)
                unique_results.append(result)

        # Log results by strategy
        strategy_counts = {}
        for result in unique_results:
            strategy = result.get("strategy", "unknown")
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1

        logger.info(f"Retrieved {len(unique_results)} unique chunks by strategy: {strategy_counts}")

        # Sort by similarity score and limit to requested number
        sorted_results = sorted(unique_results, key=lambda x: x.get("similarity", 0), reverse=True)
        final_results = sorted_results[:limit]

        return final_results
    except Exception as e:
        logger.error(f"Error in hybrid search: {str(e)}")
        raise ValueError(f"Error retrieving chunks: {str(e)}")

def create_prompt_for_section(section_id: str, section_info: Dict, chunks: List[Dict], user_profile=None):
    """
    Create a prompt for generating content for a specific report section
    based on the retrieved chunks with improved context formatting and organization
    
    UPDATED: Now uses enhanced AD-specific prompts that eliminate placeholders and generate complete content

    Args:
        section_id: ID of the section to generate
        section_info: Information about the section
        chunks: List of retrieved document chunks
        user_profile: Optional user profile information to include in the prompt
    """
    
    # Import the enhanced prompt function
    try:
        from app.tasks.generate_report_tasks.ad_report_task import create_ad_specific_prompt
        # Use enhanced prompt system with context from chunks
        enhanced_prompt = create_ad_specific_prompt(section_id, section_info, user_profile)
        
        # Add document context to the enhanced prompt using existing formatting
        if chunks:
            # Use existing chunk formatting logic
            chunks_by_doc = {}
            for chunk in chunks:
                doc_id = chunk.get('document_id', 'unknown')
                if doc_id not in chunks_by_doc:
                    chunks_by_doc[doc_id] = []
                chunks_by_doc[doc_id].append(chunk)
            
            formatted_chunks = []
            for doc_id, doc_chunks in chunks_by_doc.items():
                if doc_chunks and 'metadata' in doc_chunks[0] and 'document_name' in doc_chunks[0]['metadata']:
                    doc_name = doc_chunks[0]['metadata']['document_name']
                else:
                    doc_name = f"Document {doc_id[:8]}"
                    
                for i, chunk in enumerate(doc_chunks):
                    chunk_header = f"Document: {doc_name} (Fragment {i+1})"
                    formatted_chunks.append(f"{chunk_header}\n\n{chunk['content']}")
            
            context = "\n\n" + "="*50 + "\n\n".join(formatted_chunks) + "\n\n" + "="*50 + "\n\n"
            enhanced_prompt += f"\n\nDOCUMENT CONTEXT:\n{context}\n\nGebruik bovenstaande informatie uit de documenten als basis voor de sectie."
        else:
            enhanced_prompt += "\n\nGEEN DOCUMENTEN BESCHIKBAAR: Genereer realistische, professionele content op basis van algemene AD-praktijk."
        
        return enhanced_prompt
    except ImportError:
        print("Warning: Could not import enhanced prompt system, using fallback")
        # Fallback to original logic if import fails - create basic fallback prompt
        context = ""
        if chunks:
            formatted_chunks = []
            for i, chunk in enumerate(chunks):
                formatted_chunks.append(f"Fragment {i+1}:\n{chunk['content']}")
            context = "\n\n".join(formatted_chunks)
        
        fallback_prompt = f"""
        Schrijf professionele content voor sectie '{section_id}' van een arbeidsdeskundig rapport.
        Gebruik de onderstaande documentinformatie:
        
        {context}
        
        Schrijf in Nederlandse zakelijke stijl, objectief en gebaseerd op feiten.
        """
        return fallback_prompt

def generate_content_with_llm(prompt: str):
    """
    Generate content using the configured LLM provider with optimized settings
    and robust error handling with multiple fallback strategies
    """
    # Log the attempt
    print(f"Generating content with {settings.LLM_PROVIDER}, prompt length: {len(prompt)}")
    
    # First attempt: Use most permissive settings
    try:
        # Create an LLM instance with our provider-agnostic interface
        model = create_llm_instance(
            temperature=0.1,
            max_tokens=4096,
            dangerous_content_level="BLOCK_NONE"
        )
        
        # Neutral professional system instruction
        system_instruction = (
            "Je bent een professionele tekstschrijver die objectieve rapporten maakt. "
            "Schrijf in een zakelijke stijl gebaseerd op feitelijke informatie. "
            "Vermijd gevoelige details of specifieke medische adviezen."
        )
        
        # Medical terminology is essential for labor expert reports - no filtering needed
        safe_prompt = prompt
        
        # Generate content
        print(f"Attempt 1: Using {settings.LLM_PROVIDER} with permissive settings")
        response = model.generate_content(
            [
                {"role": "system", "parts": [system_instruction]},
                {"role": "user", "parts": [safe_prompt]}
            ]
        )
        
        # Check for blocking
        if hasattr(response, 'prompt_feedback') and response.prompt_feedback and response.prompt_feedback.block_reason:
            block_reason = response.prompt_feedback.block_reason
            print(f"First attempt blocked: {block_reason}")
            # Avoid propagating the specific block reason
            print(f"Content blocked: {block_reason} - Using generic message instead")
            raise Exception("Content generation blocked")
            
        # Validate response
        if response.text and len(response.text.strip()) >= 50:
            print("First attempt successful")
            return response.text
        else:
            print("First attempt returned empty or too short content")
            print("Generated content too short - Using generic message instead")
            raise Exception("Content generation failed")
            
    except Exception as first_error:
        print(f"First attempt failed: {str(first_error)}")
        
        # Extract only the chunks content without metadata to reduce sensitive context
        chunks_content = ""
        try:
            # Look for document content between separators in the prompt
            if "=" * 50 in prompt:
                chunks_sections = prompt.split("=" * 50)
                if len(chunks_sections) >= 3:
                    chunks_content = chunks_sections[1].strip()
            
            # Second attempt: Try with very simplified prompt and reduced context
            print("Attempt 2: Using extremely simplified approach")
            
            # Even more simplified system instruction
            basic_system = "Je bent een professionele tekstschrijver."
            
            # Create a very basic prompt without specific terminology
            basic_prompt = "Maak een objectieve samenvatting van de volgende informatie voor een zakelijk rapport."
            
            # Take only first 5000 characters if chunks content is very long
            minimal_context = chunks_content[:5000] if chunks_content else "Informatie over werksituatie en mogelijkheden."
            
            basic_full = f"{basic_prompt}\n\n{minimal_context}"
            
            # Try with even more permissive settings
            model = create_llm_instance(
                temperature=0.0,  # Completely factual
                max_tokens=2048,
                dangerous_content_level="BLOCK_NONE"
            )
            
            # Generate with minimal prompt
            basic_response = model.generate_content(
                [
                    {"role": "system", "parts": [basic_system]},
                    {"role": "user", "parts": [basic_full]}
                ]
            )
            
            if basic_response.text and len(basic_response.text.strip()) >= 30:
                print("Second attempt successful")
                return basic_response.text
            else:
                raise ValueError("Second attempt returned empty or too short content")
                
        except Exception as second_error:
            print(f"Second attempt failed: {str(second_error)}")
            
            # Third attempt: Fallback to static content as last resort
            print("All attempts failed, using static fallback content")
            
            # Parse the section prompt to understand what kind of content is needed
            section_type = "general"
            if "samenvatting" in prompt.lower():
                section_type = "samenvatting"
            elif "belastbaarheid" in prompt.lower() or "mogelijkheden" in prompt.lower() or "werkmogelijkheden" in prompt.lower():
                section_type = "belastbaarheid"
            elif "visie" in prompt.lower() or "professionele visie" in prompt.lower():
                section_type = "visie"
            elif "matching" in prompt.lower() or "passend werk" in prompt.lower() or "criteria" in prompt.lower():
                section_type = "matching"
                
            # Provide helpful fallback content based on section type
            fallback_messages = {
                "samenvatting": """
                Op basis van de beschikbare documenten is een objectieve en feitelijke samenvatting samengesteld. 
                De gegevens tonen aan dat er sprake is van een werksituatie waarbij bepaalde factoren van invloed 
                zijn op het functioneren. Het dossier bevat informatie over de huidige werkomstandigheden en 
                mogelijkheden voor de toekomst.
                """,
                "belastbaarheid": """
                De beschikbare gegevens geven inzicht in de mogelijkheden en aandachtspunten voor werkzaamheden. 
                Er zijn zowel fysieke, mentale als sociale factoren die invloed hebben op de werksituatie. 
                Met de juiste aanpassingen en voorwaarden zijn er diverse mogelijkheden voor passende werkzaamheden.
                """,
                "visie": """
                Vanuit professioneel perspectief kan gesteld worden dat er kansen zijn voor werkhervatting 
                met inachtneming van de geÃ¯dentificeerde aandachtspunten. Een gefaseerde opbouw van werkzaamheden 
                met de juiste ondersteuning biedt perspectief. Het is belangrijk om rekening te houden met individuele 
                factoren en werkomstandigheden.
                """,
                "matching": """
                Voor het vinden van passend werk zijn de volgende criteria van belang:
                
                Fysieke werkomgeving:
                - Toegankelijke werkplek (E)
                - Ergonomisch verantwoorde inrichting (E)
                - Mogelijkheid tot afwisseling in houding (W)
                
                Taakinhoud:
                - Taken met afwisselende belasting (E)
                - Werkzaamheden met eigen regie op tempo (E)
                - Taken passend bij ervaring en opleiding (W)
                
                Werktijden:
                - Regelmatige werktijden (E)
                - Flexibiliteit in planning (W)
                
                Sociale werkomgeving:
                - Ondersteunende collega's (W)
                - Begripvolle leidinggevende (E)
                """,
                "general": """
                Op basis van de beschikbare documenten is deze sectie samengesteld met objectieve en feitelijke informatie. 
                De gegevens zijn zorgvuldig geanalyseerd en in een professioneel kader geplaatst. 
                Voor specifieke details over deze sectie is het raadzaam aanvullende informatie te verzamelen.
                """
            }
            
            return fallback_messages[section_type]
    
    # This should never be reached due to the fallbacks above        
    return "Op basis van de beschikbare documenten is een objectieve analyse gemaakt. Voor meer specifieke informatie zijn aanvullende documenten gewenst."

async def generate_content_for_section(section_id: str, section_info: Dict, document_ids: List[str], case_id: str, user_profile=None):
    """
    Complete RAG pipeline for generating content for a specific report section

    Args:
        section_id: ID of the section to generate
        section_info: Information about the section from the template
        document_ids: List of document IDs to use for retrieval
        case_id: ID of the case this report belongs to
        user_profile: Optional user profile information to include in the prompt
    """
    # Get relevant chunks using hybrid search
    # TODO: Cache chunks retrieval for section_id + document_ids + case_id combination
    chunks = await get_relevant_chunks(section_id, document_ids, case_id)
    
    # If no chunks were found, return a more detailed error message
    if not chunks:
        # Check if this is due to missing embeddings
        missing_embeddings = True
        for document_id in document_ids:
            document = db_service.get_row_by_id("document", document_id)
            if document:
                document_metadata = document.get("metadata", {})
                if isinstance(document_metadata, str):
                    import json
                    document_metadata = json.loads(document_metadata)
                
                # If any document has embeddings, then the issue is not missing embeddings
                if document_metadata.get("embeddings_available", False):
                    missing_embeddings = False
                    break
        
        # Provide a specific error message based on the issue
        if missing_embeddings:
            error_msg = "Kan deze sectie niet genereren omdat er geen embeddings beschikbaar zijn. De documenten worden nog verwerkt of er is een probleem met de verwerking."
            logger.warning(f"Missing embeddings for documents: {document_ids}")
        else:
            error_msg = "Onvoldoende relevante gegevens beschikbaar om deze sectie te genereren."
            
        return {
            "content": error_msg,
            "chunk_ids": [],
            "prompt": "",
            "error": "missing_embeddings" if missing_embeddings else "no_relevant_chunks"
        }
    
    # Create prompt
    # TODO: Cache prompt generation for section_id + section_info + chunk_ids combination
    prompt = create_prompt_for_section(section_id, section_info, chunks, user_profile)
    
    # Generate content using LLM with comprehensive error handling
    print(f"Attempting to generate content for section: {section_id}")
    try:
        # TODO: Cache LLM generated content for prompt hash (with TTL for freshness)
        content = generate_content_with_llm(prompt)
        print(f"Successfully generated content for section {section_id}")
        
        # Quality control validation
        try:
            quality_controller = AutomaticQualityController()
            
            # Quick quality assessment (non-blocking)
            quality_result = quality_controller.assess_content_quality(
                content=content,
                section_type=section_id,
                original_prompt=prompt,
                source_chunks=[chunk["content"] for chunk in chunks]
            )
            
            # Log quality assessment
            print(f"Quality score for {section_id}: {quality_result.overall_score:.2f}")
            
            # If quality is very low, attempt improvement
            if quality_result.overall_score < 0.6:
                print(f"Low quality content detected ({quality_result.overall_score:.2f}), attempting improvement...")
                improved_content = quality_controller.improve_content(
                    content=content,
                    quality_report=quality_result,
                    section_type=section_id
                )
                
                if improved_content and len(improved_content.strip()) > len(content.strip()) * 0.5:
                    content = improved_content
                    print(f"Content improved for section {section_id}")
                    
        except Exception as qc_error:
            # Quality control is enhancement, not requirement - log but continue
            print(f"Quality control error for {section_id}: {qc_error}")
        
        # Calculate strategy statistics
        strategy_counts = {}
        for chunk in chunks:
            strategy = chunk.get("strategy", "unknown")
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1

        return {
            "content": content,
            "chunk_ids": [chunk["id"] for chunk in chunks],
            "prompt": prompt,
            "chunk_stats": {
                "strategy_counts": strategy_counts,
                "total_chunks": len(chunks)
            }
        }
    except Exception as e:
        error_msg = str(e)
        print(f"Error in generate_content_with_llm despite fallbacks: {error_msg}")
        
        # Final fallback if all else fails - provide static generic content
        section_type = "general"
        if section_id == "samenvatting":
            section_type = "samenvatting"
        elif section_id == "belastbaarheid":
            section_type = "belastbaarheid"
        elif section_id == "visie_ad":
            section_type = "visie"
        elif section_id == "matching":
            section_type = "matching"
            
        # Provide helpful fallback content based on section type
        fallback_messages = {
            "samenvatting": """
            Op basis van de beschikbare documenten is een objectieve en feitelijke samenvatting samengesteld. 
            De gegevens tonen aan dat er sprake is van een werksituatie waarbij bepaalde factoren van invloed 
            zijn op het functioneren. Het dossier bevat informatie over de huidige werkomstandigheden en 
            mogelijkheden voor de toekomst.
            """,
            "belastbaarheid": """
            De beschikbare gegevens geven inzicht in de mogelijkheden en aandachtspunten voor werkzaamheden. 
            Er zijn zowel fysieke, mentale als sociale factoren die invloed hebben op de werksituatie. 
            Met de juiste aanpassingen en voorwaarden zijn er diverse mogelijkheden voor passende werkzaamheden.
            """,
            "visie": """
            Vanuit professioneel perspectief kan gesteld worden dat er kansen zijn voor werkhervatting 
            met inachtneming van de geÃ¯dentificeerde aandachtspunten. Een gefaseerde opbouw van werkzaamheden 
            met de juiste ondersteuning biedt perspectief. Het is belangrijk om rekening te houden met individuele 
            factoren en werkomstandigheden.
            """,
            "matching": """
            Voor het vinden van passend werk zijn de volgende criteria van belang:
            
            Fysieke werkomgeving:
            - Toegankelijke werkplek (E)
            - Ergonomisch verantwoorde inrichting (E)
            - Mogelijkheid tot afwisseling in houding (W)
            
            Taakinhoud:
            - Taken met afwisselende belasting (E)
            - Werkzaamheden met eigen regie op tempo (E)
            - Taken passend bij ervaring en opleiding (W)
            
            Werktijden:
            - Regelmatige werktijden (E)
            - Flexibiliteit in planning (W)
            
            Sociale werkomgeving:
            - Ondersteunende collega's (W)
            - Begripvolle leidinggevende (E)
            """,
            "general": """
            Op basis van de beschikbare documenten is deze sectie samengesteld met objectieve en feitelijke informatie. 
            De gegevens zijn zorgvuldig geanalyseerd en in een professioneel kader geplaatst. 
            Voor specifieke details over deze sectie is het raadzaam aanvullende informatie te verzamelen.
            """
        }
        
        # Log that we're using the absolute last resort fallback
        print(f"Using static fallback content for section {section_id}")
        
        # Calculate strategy statistics even for fallback
        strategy_counts = {}
        for chunk in chunks:
            strategy = chunk.get("strategy", "unknown")
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1

        return {
            "content": fallback_messages[section_type],
            "chunk_ids": [chunk["id"] for chunk in chunks],
            "prompt": prompt,
            "chunk_stats": {
                "strategy_counts": strategy_counts,
                "total_chunks": len(chunks),
                "fallback_used": True
            },
            "error": "Used static fallback content after all generation attempts failed: " + error_msg
        }
    
    # Calculate strategy statistics
    strategy_counts = {}
    for chunk in chunks:
        strategy = chunk.get("strategy", "unknown")
        strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1

    # Return generated content and metadata
    return {
        "content": content,
        "chunk_ids": [chunk["id"] for chunk in chunks],
        "prompt": prompt,
        "chunk_stats": {
            "strategy_counts": strategy_counts,
            "total_chunks": len(chunks)
        }
    }